{
    "metadata": {
        "language_info": {
            "name": "python", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "file_extension": ".py", 
            "version": "2.7.11"
        }, 
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.1", 
            "name": "python2-spark21", 
            "language": "python"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "metadata": {}, 
            "source": "# Python DL_NLP Crash Course_Part 5: Language Modelling with LSTM\n\n## Full Day Workshop for user learn Data Science with Python\n### 2017 Dec Timothy CL Lam\nThis is meant for internal usage, part of contents copied externally, not for commercial purpose\n", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# The code was removed by DSX for sharing.", 
            "metadata": {}, 
            "execution_count": 3, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>YEAR</th>\n      <th>QUARTER</th>\n      <th>QUARTER_YEAR</th>\n      <th>CHURN_RATE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2014</td>\n      <td>1</td>\n      <td>1Q14</td>\n      <td>18.1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2014</td>\n      <td>2</td>\n      <td>2Q14</td>\n      <td>18.7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2014</td>\n      <td>3</td>\n      <td>3Q14</td>\n      <td>19.3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2014</td>\n      <td>4</td>\n      <td>4Q14</td>\n      <td>19.9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2015</td>\n      <td>1</td>\n      <td>1Q15</td>\n      <td>20.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "   YEAR  QUARTER QUARTER_YEAR  CHURN_RATE\n0  2014        1         1Q14        18.1\n1  2014        2         2Q14        18.7\n2  2014        3         3Q14        19.3\n3  2014        4         4Q14        19.9\n4  2015        1         1Q15        20.5"
                    }, 
                    "metadata": {}, 
                    "execution_count": 3, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## Language Model\n- A language model predicts the next word in the sequence based on the speci\fc words that have\ncome before it in the sequence. ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "# Character-Based Neural Language Model\n\n- It is possible to develop language models at the character\nlevel using neural networks. \n- The benefit of character-based language models is their small\nvocabulary and \nexibility in handling any words, punctuation, and other document structure.\n- This comes at the cost of requiring larger models that are slower to train.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "# Sing a song of Experience\n- The nursery rhyme Sing a Song of Sixpence is well known in the west. \n- The first verse is common,\nbut there is also a 4 verse version that we will use to develop our character-based language\nmodel.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "Sing a song of sixpence,\nA pocket full of rye.\nFour and twenty blackbirds,\nBaked in a pie.\nWhen the pie was opened\nThe birds began to sing;\nWasn't that a dainty dish,\nTo set before the king.\nThe king was in his counting house,\nCounting out his money;\nThe queen was in the parlour,\nEating bread and honey.\nThe maid was in the garden,\nHanging out the clothes,\nWhen down came a blackbird\nAnd pecked off her nose.", 
            "metadata": {}, 
            "execution_count": 1, 
            "cell_type": "code", 
            "outputs": [
                {
                    "ename": "SyntaxError", 
                    "output_type": "error", 
                    "evalue": "invalid syntax (<ipython-input-1-67d65fddc005>, line 1)", 
                    "traceback": [
                        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-67d65fddc005>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Sing a song of sixpence,\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
                    ]
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "# Data Preparation\n## Language Model Design\n- A language model must be trained on the text, and in the case of a character-based language\nmodel, the input and output sequences must be characters. \n- The number of characters used\nas input will also define the number of characters that will need to be provided to the model\nin order to elicit the first predicted character.\n- Longer sequences o\u000ber more context for the model to learn what character to output next\nbut take longer to train and impose more burden on seeding the model when generating text.\n- We will use an arbitrary length of 10 characters for this model.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "#### We must load the text into memory so that we can work with it. Below is a function named load doc()", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# load doc into memory\ndef load_doc(file):\n    # open the file as read only\n    file = open(filename, 'r')\n    # read all text\n    text = file.read()\n    # close the file\n    file.close()\n    return text", 
            "metadata": {}, 
            "execution_count": 6, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# The code was removed by DSX for sharing.", 
            "metadata": {}, 
            "execution_count": 7, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "import requests, StringIO, pandas as pd, json, re", 
            "metadata": {}, 
            "execution_count": 8, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "def get_file_content(credentials):\n    \"\"\"For given credentials, this functions returns a StringIO object containing the file content.\"\"\"\n    \n    url1 = ''.join([credentials['auth_url'], '/v3/auth/tokens'])\n    data = {'auth': {'identity': {'methods': ['password'],\n            'password': {'user': {'name': credentials['username'],'domain': {'id': credentials['domain_id']},\n            'password': credentials['password']}}}}}\n    headers1 = {'Content-Type': 'application/json'}\n    resp1 = requests.post(url=url1, data=json.dumps(data), headers=headers1)\n    resp1_body = resp1.json()    \n    for e1 in resp1_body['token']['catalog']:\n        if(e1['type']=='object-store'):\n            for e2 in e1['endpoints']:\n                if(e2['interface']=='public'and e2['region']==credentials['region']):\n                    url2 = ''.join([e2['url'],'/', credentials['container'], '/', credentials['filename']])\n    s_subject_token = resp1.headers['x-subject-token']\n    headers2 = {'X-Auth-Token': s_subject_token, 'accept': 'application/json'}\n    resp2 = requests.get(url=url2, headers=headers2)\n    return StringIO.StringIO(resp2.content)", 
            "metadata": {}, 
            "execution_count": 105, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "content_string = get_file_content(credentials_1)\nraw_text = content_string.getvalue()\nprint raw_text", 
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": 106, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "\r\n"
                }
            ]
        }, 
        {
            "source": "raw_text = raw_text[:-3]", 
            "metadata": {}, 
            "execution_count": 107, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "raw_text[:]", 
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": 108, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "\"Sing a song of sixpence,\\rA pocket full of rye.\\rFour and twenty blackbirds,\\rBaked in a pie.\\rWhen the pie was opened\\rThe birds began to sing;\\rWasn' t that a dainty dish,\\rTo set before the king.\\rThe king was in his counting house,\\rCounting out his money;\\rThe queen was in the parlour,\\rEating bread and honey.\\rThe maid was in the garden,\\rHanging out the clothes,\\rWhen down came a blackbird\\rAnd pecked off her nose\""
                    }, 
                    "metadata": {}, 
                    "execution_count": 108, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "## Clean Text\n- we will strip all of the new line characters \n- so that we have one long sequence of characters\nseparated only by white space.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# clean\ntokens = raw_text.split()\nraw_text = ' '.join(tokens)", 
            "metadata": {}, 
            "execution_count": 148, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "raw_text[:]", 
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": 149, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "\"Sing a song of sixpence, A pocket full of rye. Four and twenty blackbirds, Baked in a pie. When the pie was opened The birds began to sing; Wasn' t that a dainty dish, To set before the king. The king was in his counting house, Counting out his money; The queen was in the parlour, Eating bread and honey. The maid was in the garden, Hanging out the clothes, When down came a blackbird And pecked off her nose\""
                    }, 
                    "metadata": {}, 
                    "execution_count": 149, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "### Tips\n- You may want to explore other methods for data cleaning, \n- such as normalizing the case to\nlowercase or removing punctuation in an effort \n- to reduce the final vocabulary size and develop a\nsmaller and leaner model.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## Create Sequences\n- Now that we have a long list of characters, \n- we can create our input-output sequences used to\ntrain the model. \n- Each input sequence will be 10 characters with one output character, \n- making\neach sequence 11 characters long", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# organize into sequences of characters\nchars = sorted(list(set(raw_text)))\nmapping = dict((c, i) for i, c in enumerate(chars))\nlength = 10\n", 
            "metadata": {}, 
            "execution_count": 150, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "sequences = list()\nfor i in range(length, len(raw_text)):\n    # select sequence of tokens\n    seq = raw_text[i-length:i+1]\n    # store\n    sequences.append(seq)\nprint('Total Sequences: %d' % len(sequences))\n", 
            "metadata": {}, 
            "execution_count": 151, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Total Sequences: 399\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "#### We can create the sequences by enumerating the characters in the text, starting at the 11th character at index 10.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "sequences[:]", 
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": 152, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "['Sing a song',\n 'ing a song ',\n 'ng a song o',\n 'g a song of',\n ' a song of ',\n 'a song of s',\n ' song of si',\n 'song of six',\n 'ong of sixp',\n 'ng of sixpe',\n 'g of sixpen',\n ' of sixpenc',\n 'of sixpence',\n 'f sixpence,',\n ' sixpence, ',\n 'sixpence, A',\n 'ixpence, A ',\n 'xpence, A p',\n 'pence, A po',\n 'ence, A poc',\n 'nce, A pock',\n 'ce, A pocke',\n 'e, A pocket',\n ', A pocket ',\n ' A pocket f',\n 'A pocket fu',\n ' pocket ful',\n 'pocket full',\n 'ocket full ',\n 'cket full o',\n 'ket full of',\n 'et full of ',\n 't full of r',\n ' full of ry',\n 'full of rye',\n 'ull of rye.',\n 'll of rye. ',\n 'l of rye. F',\n ' of rye. Fo',\n 'of rye. Fou',\n 'f rye. Four',\n ' rye. Four ',\n 'rye. Four a',\n 'ye. Four an',\n 'e. Four and',\n '. Four and ',\n ' Four and t',\n 'Four and tw',\n 'our and twe',\n 'ur and twen',\n 'r and twent',\n ' and twenty',\n 'and twenty ',\n 'nd twenty b',\n 'd twenty bl',\n ' twenty bla',\n 'twenty blac',\n 'wenty black',\n 'enty blackb',\n 'nty blackbi',\n 'ty blackbir',\n 'y blackbird',\n ' blackbirds',\n 'blackbirds,',\n 'lackbirds, ',\n 'ackbirds, B',\n 'ckbirds, Ba',\n 'kbirds, Bak',\n 'birds, Bake',\n 'irds, Baked',\n 'rds, Baked ',\n 'ds, Baked i',\n 's, Baked in',\n ', Baked in ',\n ' Baked in a',\n 'Baked in a ',\n 'aked in a p',\n 'ked in a pi',\n 'ed in a pie',\n 'd in a pie.',\n ' in a pie. ',\n 'in a pie. W',\n 'n a pie. Wh',\n ' a pie. Whe',\n 'a pie. When',\n ' pie. When ',\n 'pie. When t',\n 'ie. When th',\n 'e. When the',\n '. When the ',\n ' When the p',\n 'When the pi',\n 'hen the pie',\n 'en the pie ',\n 'n the pie w',\n ' the pie wa',\n 'the pie was',\n 'he pie was ',\n 'e pie was o',\n ' pie was op',\n 'pie was ope',\n 'ie was open',\n 'e was opene',\n ' was opened',\n 'was opened ',\n 'as opened T',\n 's opened Th',\n ' opened The',\n 'opened The ',\n 'pened The b',\n 'ened The bi',\n 'ned The bir',\n 'ed The bird',\n 'd The birds',\n ' The birds ',\n 'The birds b',\n 'he birds be',\n 'e birds beg',\n ' birds bega',\n 'birds began',\n 'irds began ',\n 'rds began t',\n 'ds began to',\n 's began to ',\n ' began to s',\n 'began to si',\n 'egan to sin',\n 'gan to sing',\n 'an to sing;',\n 'n to sing; ',\n ' to sing; W',\n 'to sing; Wa',\n 'o sing; Was',\n ' sing; Wasn',\n \"sing; Wasn'\",\n \"ing; Wasn' \",\n \"ng; Wasn' t\",\n \"g; Wasn' t \",\n \"; Wasn' t t\",\n \" Wasn' t th\",\n \"Wasn' t tha\",\n \"asn' t that\",\n \"sn' t that \",\n \"n' t that a\",\n \"' t that a \",\n ' t that a d',\n 't that a da',\n ' that a dai',\n 'that a dain',\n 'hat a daint',\n 'at a dainty',\n 't a dainty ',\n ' a dainty d',\n 'a dainty di',\n ' dainty dis',\n 'dainty dish',\n 'ainty dish,',\n 'inty dish, ',\n 'nty dish, T',\n 'ty dish, To',\n 'y dish, To ',\n ' dish, To s',\n 'dish, To se',\n 'ish, To set',\n 'sh, To set ',\n 'h, To set b',\n ', To set be',\n ' To set bef',\n 'To set befo',\n 'o set befor',\n ' set before',\n 'set before ',\n 'et before t',\n 't before th',\n ' before the',\n 'before the ',\n 'efore the k',\n 'fore the ki',\n 'ore the kin',\n 're the king',\n 'e the king.',\n ' the king. ',\n 'the king. T',\n 'he king. Th',\n 'e king. The',\n ' king. The ',\n 'king. The k',\n 'ing. The ki',\n 'ng. The kin',\n 'g. The king',\n '. The king ',\n ' The king w',\n 'The king wa',\n 'he king was',\n 'e king was ',\n ' king was i',\n 'king was in',\n 'ing was in ',\n 'ng was in h',\n 'g was in hi',\n ' was in his',\n 'was in his ',\n 'as in his c',\n 's in his co',\n ' in his cou',\n 'in his coun',\n 'n his count',\n ' his counti',\n 'his countin',\n 'is counting',\n 's counting ',\n ' counting h',\n 'counting ho',\n 'ounting hou',\n 'unting hous',\n 'nting house',\n 'ting house,',\n 'ing house, ',\n 'ng house, C',\n 'g house, Co',\n ' house, Cou',\n 'house, Coun',\n 'ouse, Count',\n 'use, Counti',\n 'se, Countin',\n 'e, Counting',\n ', Counting ',\n ' Counting o',\n 'Counting ou',\n 'ounting out',\n 'unting out ',\n 'nting out h',\n 'ting out hi',\n 'ing out his',\n 'ng out his ',\n 'g out his m',\n ' out his mo',\n 'out his mon',\n 'ut his mone',\n 't his money',\n ' his money;',\n 'his money; ',\n 'is money; T',\n 's money; Th',\n ' money; The',\n 'money; The ',\n 'oney; The q',\n 'ney; The qu',\n 'ey; The que',\n 'y; The quee',\n '; The queen',\n ' The queen ',\n 'The queen w',\n 'he queen wa',\n 'e queen was',\n ' queen was ',\n 'queen was i',\n 'ueen was in',\n 'een was in ',\n 'en was in t',\n 'n was in th',\n ' was in the',\n 'was in the ',\n 'as in the p',\n 's in the pa',\n ' in the par',\n 'in the parl',\n 'n the parlo',\n ' the parlou',\n 'the parlour',\n 'he parlour,',\n 'e parlour, ',\n ' parlour, E',\n 'parlour, Ea',\n 'arlour, Eat',\n 'rlour, Eati',\n 'lour, Eatin',\n 'our, Eating',\n 'ur, Eating ',\n 'r, Eating b',\n ', Eating br',\n ' Eating bre',\n 'Eating brea',\n 'ating bread',\n 'ting bread ',\n 'ing bread a',\n 'ng bread an',\n 'g bread and',\n ' bread and ',\n 'bread and h',\n 'read and ho',\n 'ead and hon',\n 'ad and hone',\n 'd and honey',\n ' and honey.',\n 'and honey. ',\n 'nd honey. T',\n 'd honey. Th',\n ' honey. The',\n 'honey. The ',\n 'oney. The m',\n 'ney. The ma',\n 'ey. The mai',\n 'y. The maid',\n '. The maid ',\n ' The maid w',\n 'The maid wa',\n 'he maid was',\n 'e maid was ',\n ' maid was i',\n 'maid was in',\n 'aid was in ',\n 'id was in t',\n 'd was in th',\n ' was in the',\n 'was in the ',\n 'as in the g',\n 's in the ga',\n ' in the gar',\n 'in the gard',\n 'n the garde',\n ' the garden',\n 'the garden,',\n 'he garden, ',\n 'e garden, H',\n ' garden, Ha',\n 'garden, Han',\n 'arden, Hang',\n 'rden, Hangi',\n 'den, Hangin',\n 'en, Hanging',\n 'n, Hanging ',\n ', Hanging o',\n ' Hanging ou',\n 'Hanging out',\n 'anging out ',\n 'nging out t',\n 'ging out th',\n 'ing out the',\n 'ng out the ',\n 'g out the c',\n ' out the cl',\n 'out the clo',\n 'ut the clot',\n 't the cloth',\n ' the clothe',\n 'the clothes',\n 'he clothes,',\n 'e clothes, ',\n ' clothes, W',\n 'clothes, Wh',\n 'lothes, Whe',\n 'othes, When',\n 'thes, When ',\n 'hes, When d',\n 'es, When do',\n 's, When dow',\n ', When down',\n ' When down ',\n 'When down c',\n 'hen down ca',\n 'en down cam',\n 'n down came',\n ' down came ',\n 'down came a',\n 'own came a ',\n 'wn came a b',\n 'n came a bl',\n ' came a bla',\n 'came a blac',\n 'ame a black',\n 'me a blackb',\n 'e a blackbi',\n ' a blackbir',\n 'a blackbird',\n ' blackbird ',\n 'blackbird A',\n 'lackbird An',\n 'ackbird And',\n 'ckbird And ',\n 'kbird And p',\n 'bird And pe',\n 'ird And pec',\n 'rd And peck',\n 'd And pecke',\n ' And pecked',\n 'And pecked ',\n 'nd pecked o',\n 'd pecked of',\n ' pecked off',\n 'pecked off ',\n 'ecked off h',\n 'cked off he',\n 'ked off her',\n 'ed off her ',\n 'd off her n',\n ' off her no',\n 'off her nos',\n 'ff her nose']"
                    }, 
                    "metadata": {}, 
                    "execution_count": 152, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "# save tokens to file, one dialog per line\ndef save_doc(lines, filename):\n    data = '\\n'.join(lines)\n    file = open(filename, 'w')\n    file.write(data)\n    file.close()\n    \n# save sequences to file\nout_filename = 'char_sequences.txt'\nsave_doc(sequences, out_filename)", 
            "metadata": {}, 
            "execution_count": 153, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## Train Language Model\n- Coming section is to develop a neural language model for the prepared sequence data. \n- The\nmodel will read encoded characters and predict the next character in the sequence.\n- A Long\nShort-Term Memory recurrent neural network hidden layer will be used \n- to learn the context\nfrom the input sequence in order to make the predictions.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# load doc into memory\ndef load_doc(filename):\n    # open the file as read only\n    file = open(filename, 'r')\n    # read all text\n    text = file.read()\n    # close the file\n    file.close()\n    return text\n\n# load\nin_filename = 'char_sequences.txt'\nraw_text = load_doc(in_filename)\nlines = raw_text.split('\\n')", 
            "metadata": {}, 
            "execution_count": 154, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "from numpy import array\nfrom pickle import dump\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.utils.vis_utils import plot_model", 
            "metadata": {}, 
            "execution_count": 156, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## Encode Sequences\n- The sequences of characters must be encoded as integers. \n- This means that each unique character\nwill be assigned a specific integer value and each sequence of characters will be encoded as a\nsequence of integers. \n- We can create the mapping given a sorted set of unique characters in the\nraw input data. \n- The mapping is a dictionary of character values to integer values.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# integer encode sequences of characters\nchars = sorted(list(set(raw_text)))\nmapping = dict((c, i) for i, c in enumerate(chars))", 
            "metadata": {}, 
            "execution_count": 157, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## Character Mapping & Vocab Size\n- Next, we can process each sequence of characters one at a time \n- and use the dictionary mapping to look up the integer value for each character\n- The result is a list of integer lists. \n- We need to know the size of the vocabulary later.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "sequences = list()\nfor line in lines:\n    # integer encode line\n    encoded_seq = [mapping[char] for char in line]\n    # store\n    sequences.append(encoded_seq)\n# vocabulary size\nvocab_size = len(mapping)\nprint('Vocabulary Size: %d' % vocab_size)", 
            "metadata": {}, 
            "execution_count": 158, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Vocabulary Size: 38\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "## Define Input & output\n- Now that the sequences have been integer encoded, \n- we can separate the columns into input and\noutput sequences of characters. \n- We can do this using a simple array slice.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# separate into input and output\nsequences = array(sequences)\nX, y = sequences[:,:-1], sequences[:,-1]\nsequences = [to_categorical(x, num_classes=vocab_size) for x in X]\nX = array(sequences)\ny = to_categorical(y, num_classes=vocab_size)\n\n", 
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": 159, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## One Hot Encoded\n- Next, we need to one hot encode each character.\n- That is, each character becomes a vector as\nlong as the vocabulary (38 elements) \n- with a 1 marked for the speci\fc character. \n- This provides\na more precise input representation for the network.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## LSTM Model Setting\n- The model is defined with an input layer that takes sequences that have 10 time steps and 38\nfeatures for the one hot encoded input sequences. \n- Rather than specify these numbers, we use\nthe second and third dimensions on the X input data. \n- This is so that if we change the length of\nthe sequences or size of the vocabulary, we do not need to change the model definition. \n- The\nmodel has a single LSTM hidden layer with 75 memory cells, chosen with a little trial and\nerror. \n- The model has a fully connected output layer that outputs one vector with a probability\ndistribution across all characters in the vocabulary. \n- A softmax activation function is used on\nthe output layer to ensure the output has the properties of a probability distribution.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# define the model\ndef define_model(X):\n    model = Sequential()\n    model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n    model.add(Dense(vocab_size, activation='softmax'))\n    # compile model\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # summarize defined model\n    model.summary()\n    plot_model(model, to_file='model.png', show_shapes=True)\n    return model", 
            "metadata": {}, 
            "execution_count": 160, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## Evaluate the Model\n- The model is learning a multiclass classification problem, \n- therefore we use the categorical log\nloss intended for this type of problem. \n- The efficient Adam implementation of gradient descent\nis used to optimize the model and accuracy is reported at the end of each batch update.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# define model\nmodel = define_model(X)\n# fit model\nmodel.fit(X, y, epochs=100, verbose=2)", 
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": 161, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_4 (LSTM)                (None, 75)                34200     \n_________________________________________________________________\ndense_4 (Dense)              (None, 38)                2888      \n=================================================================\nTotal params: 37,088\nTrainable params: 37,088\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/100\n - 2s - loss: 3.6097 - acc: 0.1228\nEpoch 2/100\n - 0s - loss: 3.4844 - acc: 0.1930\nEpoch 3/100\n - 0s - loss: 3.1846 - acc: 0.1930\nEpoch 4/100\n - 0s - loss: 3.0560 - acc: 0.1930\nEpoch 5/100\n - 0s - loss: 3.0179 - acc: 0.1930\nEpoch 6/100\n - 0s - loss: 2.9946 - acc: 0.1930\nEpoch 7/100\n - 0s - loss: 2.9800 - acc: 0.1930\nEpoch 8/100\n - 0s - loss: 2.9648 - acc: 0.1930\nEpoch 9/100\n - 0s - loss: 2.9536 - acc: 0.1930\nEpoch 10/100\n - 0s - loss: 2.9390 - acc: 0.1930\nEpoch 11/100\n - 0s - loss: 2.9258 - acc: 0.1930\nEpoch 12/100\n - 0s - loss: 2.9066 - acc: 0.1930\nEpoch 13/100\n - 0s - loss: 2.8826 - acc: 0.2005\nEpoch 14/100\n - 0s - loss: 2.8692 - acc: 0.2080\nEpoch 15/100\n - 0s - loss: 2.8504 - acc: 0.2155\nEpoch 16/100\n - 0s - loss: 2.8046 - acc: 0.2030\nEpoch 17/100\n - 0s - loss: 2.7812 - acc: 0.2005\nEpoch 18/100\n - 0s - loss: 2.7545 - acc: 0.2506\nEpoch 19/100\n - 0s - loss: 2.7190 - acc: 0.2481\nEpoch 20/100\n - 0s - loss: 2.6705 - acc: 0.2481\nEpoch 21/100\n - 0s - loss: 2.6303 - acc: 0.2707\nEpoch 22/100\n - 0s - loss: 2.6048 - acc: 0.2581\nEpoch 23/100\n - 0s - loss: 2.5761 - acc: 0.2732\nEpoch 24/100\n - 0s - loss: 2.5108 - acc: 0.2832\nEpoch 25/100\n - 0s - loss: 2.4851 - acc: 0.3133\nEpoch 26/100\n - 0s - loss: 2.4403 - acc: 0.3083\nEpoch 27/100\n - 0s - loss: 2.3990 - acc: 0.3258\nEpoch 28/100\n - 0s - loss: 2.3508 - acc: 0.3383\nEpoch 29/100\n - 0s - loss: 2.3067 - acc: 0.3233\nEpoch 30/100\n - 0s - loss: 2.2590 - acc: 0.3559\nEpoch 31/100\n - 0s - loss: 2.2232 - acc: 0.3709\nEpoch 32/100\n - 0s - loss: 2.1839 - acc: 0.3659\nEpoch 33/100\n - 0s - loss: 2.1246 - acc: 0.4110\nEpoch 34/100\n - 0s - loss: 2.0946 - acc: 0.3960\nEpoch 35/100\n - 0s - loss: 2.0338 - acc: 0.4436\nEpoch 36/100\n - 0s - loss: 1.9940 - acc: 0.4185\nEpoch 37/100\n - 0s - loss: 1.9644 - acc: 0.4511\nEpoch 38/100\n - 0s - loss: 1.9156 - acc: 0.4461\nEpoch 39/100\n - 0s - loss: 1.8862 - acc: 0.4286\nEpoch 40/100\n - 0s - loss: 1.8432 - acc: 0.4912\nEpoch 41/100\n - 0s - loss: 1.8059 - acc: 0.4887\nEpoch 42/100\n - 0s - loss: 1.7674 - acc: 0.5188\nEpoch 43/100\n - 0s - loss: 1.7251 - acc: 0.5138\nEpoch 44/100\n - 0s - loss: 1.6662 - acc: 0.5414\nEpoch 45/100\n - 0s - loss: 1.6475 - acc: 0.5714\nEpoch 46/100\n - 0s - loss: 1.6002 - acc: 0.5664\nEpoch 47/100\n - 0s - loss: 1.5578 - acc: 0.5840\nEpoch 48/100\n - 0s - loss: 1.5186 - acc: 0.6065\nEpoch 49/100\n - 0s - loss: 1.4939 - acc: 0.5890\nEpoch 50/100\n - 0s - loss: 1.4472 - acc: 0.6190\nEpoch 51/100\n - 0s - loss: 1.4069 - acc: 0.6366\nEpoch 52/100\n - 0s - loss: 1.3864 - acc: 0.6190\nEpoch 53/100\n - 0s - loss: 1.3406 - acc: 0.6541\nEpoch 54/100\n - 0s - loss: 1.2960 - acc: 0.6667\nEpoch 55/100\n - 0s - loss: 1.2489 - acc: 0.7018\nEpoch 56/100\n - 0s - loss: 1.2265 - acc: 0.7168\nEpoch 57/100\n - 0s - loss: 1.1803 - acc: 0.7143\nEpoch 58/100\n - 0s - loss: 1.1733 - acc: 0.7118\nEpoch 59/100\n - 0s - loss: 1.1016 - acc: 0.7494\nEpoch 60/100\n - 0s - loss: 1.0805 - acc: 0.7669\nEpoch 61/100\n - 0s - loss: 1.0328 - acc: 0.7870\nEpoch 62/100\n - 0s - loss: 1.0061 - acc: 0.7870\nEpoch 63/100\n - 0s - loss: 0.9629 - acc: 0.7794\nEpoch 64/100\n - 0s - loss: 0.9366 - acc: 0.8170\nEpoch 65/100\n - 0s - loss: 0.9106 - acc: 0.8145\nEpoch 66/100\n - 0s - loss: 0.8635 - acc: 0.8346\nEpoch 67/100\n - 0s - loss: 0.8501 - acc: 0.8271\nEpoch 68/100\n - 0s - loss: 0.8074 - acc: 0.8622\nEpoch 69/100\n - 0s - loss: 0.7827 - acc: 0.8747\nEpoch 70/100\n - 0s - loss: 0.7518 - acc: 0.8697\nEpoch 71/100\n - 0s - loss: 0.7355 - acc: 0.8697\nEpoch 72/100\n - 0s - loss: 0.7332 - acc: 0.8697\nEpoch 73/100\n - 0s - loss: 0.7107 - acc: 0.8772\nEpoch 74/100\n - 0s - loss: 0.6482 - acc: 0.9123\nEpoch 75/100\n - 0s - loss: 0.6112 - acc: 0.9398\nEpoch 76/100\n - 0s - loss: 0.5860 - acc: 0.9273\nEpoch 77/100\n - 0s - loss: 0.5563 - acc: 0.9398\nEpoch 78/100\n - 0s - loss: 0.5422 - acc: 0.9373\nEpoch 79/100\n - 0s - loss: 0.5221 - acc: 0.9549\nEpoch 80/100\n - 0s - loss: 0.5092 - acc: 0.9499\nEpoch 81/100\n - 0s - loss: 0.4984 - acc: 0.9549\nEpoch 82/100\n - 0s - loss: 0.4728 - acc: 0.9574\nEpoch 83/100\n - 0s - loss: 0.4436 - acc: 0.9749\nEpoch 84/100\n - 0s - loss: 0.4285 - acc: 0.9674\nEpoch 85/100\n - 0s - loss: 0.4130 - acc: 0.9699\nEpoch 86/100\n - 0s - loss: 0.3961 - acc: 0.9799\nEpoch 87/100\n - 0s - loss: 0.3736 - acc: 0.9774\nEpoch 88/100\n - 0s - loss: 0.3561 - acc: 0.9825\nEpoch 89/100\n - 0s - loss: 0.3432 - acc: 0.9799\nEpoch 90/100\n - 0s - loss: 0.3376 - acc: 0.9749\nEpoch 91/100\n - 0s - loss: 0.3308 - acc: 0.9875\nEpoch 92/100\n - 0s - loss: 0.3161 - acc: 0.9900\nEpoch 93/100\n - 0s - loss: 0.3022 - acc: 0.9799\nEpoch 94/100\n - 0s - loss: 0.2845 - acc: 0.9875\nEpoch 95/100\n - 0s - loss: 0.2696 - acc: 0.9850\nEpoch 96/100\n - 0s - loss: 0.2591 - acc: 0.9925\nEpoch 97/100\n - 0s - loss: 0.2505 - acc: 0.9925\nEpoch 98/100\n - 0s - loss: 0.2433 - acc: 0.9900\nEpoch 99/100\n - 0s - loss: 0.2356 - acc: 0.9850\nEpoch 100/100\n - 0s - loss: 0.2306 - acc: 0.9900\n"
                }, 
                {
                    "data": {
                        "text/plain": "<keras.callbacks.History at 0x7ff966fc6810>"
                    }, 
                    "metadata": {}, 
                    "execution_count": 161, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "## Save Model\n- After the model is fit, we save it to file for later use. \n- The Keras model API provides the save()\nfunction that we can use to save the model to a single file, \n- including weights and topology\ninformation.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "!pip install h5py", 
            "metadata": {}, 
            "execution_count": 20, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Requirement already satisfied: h5py in /gpfs/global_fs01/sym_shared/YPProdSpark/user/sbb6-28ae32a56257b0-666d72869b6d/.local/lib/python2.7/site-packages\r\nRequirement already satisfied: numpy>=1.7 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/sbb6-28ae32a56257b0-666d72869b6d/.local/lib/python2.7/site-packages (from h5py)\r\nRequirement already satisfied: six in /gpfs/global_fs01/sym_shared/YPProdSpark/user/sbb6-28ae32a56257b0-666d72869b6d/.local/lib/python2.7/site-packages (from h5py)\r\n"
                }
            ]
        }, 
        {
            "source": "import h5py", 
            "metadata": {}, 
            "execution_count": 21, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# save the model to file\nmodel.save('model.h5py')", 
            "metadata": {}, 
            "execution_count": 162, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## Save Mapping\n- We also save the mapping from characters to integers that \n- we will need to encode any input\nwhen using the model and decode any output from the model", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# save the mapping\ndump(mapping, open('mapping.pkl', 'wb'))", 
            "metadata": {}, 
            "execution_count": 163, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "# Generate Text\n- We will use the learned language model to generate new sequences of text that have the same statistical properties.\n\n## Load Model & Mapping\n- The first step is to load the model saved to the \fle model.h5. We can use the load model() function from the Keras API.\n- We also need to load the pickled dictionary for mapping characters to integers from the \fle\n- mapping.pkl. We will use the Pickle API to load the object.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "from pickle import load\nfrom keras.models import load_model\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences", 
            "metadata": {}, 
            "execution_count": 164, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# load the model\nmodel = load_model('model.h5')\n\n# load the mapping\nmapping = load(open('mapping.pkl', 'rb'))", 
            "metadata": {}, 
            "execution_count": 170, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## Generate Characters\n- We must provide sequences of 10 characters as input to the model in order to start the generation process. We will pick these manually\n- First, the sequence of characters must\nbe integer encoded using the loaded mapping.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# encode the characters as integers\n# encoded = [mapping[char] for char in in_text]", 
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": 39, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## ENcode & 3 Dimension\n- Next, the integers need to be one hot encoded using the to categorical() Keras function.\n- We also need to reshape the sequence to be 3-dimensional, as we only have one sequence and\n- LSTMs require all input to be three dimensional (samples, time steps, features).", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# one hot encode\n# encoded = to_categorical(encoded, num_classes=len(mapping))\n# encoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])", 
            "metadata": {}, 
            "execution_count": 40, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## Predict\n- We can then use the model to predict the next character in the sequence. \n- We use\npredict classes() instead of predict() to directly select the integer for the character with\nthe highest probability\n- instead of getting the full probability distribution across the entire set of\ncharacters.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# predict character\n# yhat = model.predict_classes(encoded, verbose=0)", 
            "metadata": {}, 
            "execution_count": 41, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## Decode & Pad\n- We can then decode this integer by looking up the mapping to see the character to which it maps.\n- This character can then be added to the input sequence. We then need to make sure that the\ninput sequence is 10 characters by truncating the \frst character from the input sequence text.\n- We can use the pad sequences() function from the Keras API that can perform this truncation\noperation. \n- Putting all of this together, we can de\fne a new function named generate seq()\nfor using the loaded model to generate new sequences of text", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# generate a sequence of characters with a language model\ndef generate_seq(model, mapping, seq_length, seed_text, n_chars):\n    in_text = seed_text\n    # generate a fixed number of characters\n    for _ in range(n_chars):\n        # encode the characters as integers\n        encoded = [mapping[char] for char in in_text]\n        # truncate sequences to a fixed length\n        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n        # one hot encode\n        encoded = to_categorical(encoded, num_classes=len(mapping))\n        encoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\n        # predict character\n        yhat = model.predict_classes(encoded, verbose=0)\n        # reverse map integer to character\n        out_char = ''\n        for char, index in mapping.items():\n            if index == yhat:\n                out_char = char\n                break\n    # append to input\n    in_text += char\n    return in_text", 
            "metadata": {}, 
            "execution_count": 166, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# load the model\nmodel = load_model('model.h5')\n# load the mapping\nmapping = load(open('mapping.pkl', 'rb'))\n", 
            "metadata": {}, 
            "execution_count": 171, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# test start of rhyme\nprint(generate_seq(model, mapping, 10, 'Sing a son', 20))\n# test mid-line\nprint(generate_seq(model, mapping, 10, 'king was i', 20))\n# test not in original\nprint(generate_seq(model, mapping, 10, 'hello worl', 20))", 
            "metadata": {}, 
            "execution_count": 174, 
            "cell_type": "code", 
            "outputs": [
                {
                    "ename": "ValueError", 
                    "output_type": "error", 
                    "evalue": "cannot reshape array of size 380 into shape (1,1,10)", 
                    "traceback": [
                        "\u001b[0;31m\u001b[0m", 
                        "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-174-218760d81332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test start of rhyme\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sing a so'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# test mid-line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'king was i'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# test not in original\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m<ipython-input-166-fe4e1061be41>\u001b[0m in \u001b[0;36mgenerate_seq\u001b[0;34m(model, mapping, seq_length, seed_text, n_chars)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# one hot encode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m# predict character\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 380 into shape (1,1,10)"
                    ]
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Word-Based Neural Language Model\n## Framing Language Modeling\n\n_* A statistical language model is learned from raw text and predicts the probability of the next word in the sequence given the words already present in the sequence. *_\n\nLanguage models are a key component in larger models for challenging natural language processing problems, like machine translation and speech recognition. They can also be developed as standalone models and used for generating new sequences that have the same statistical properties as the source text.\n\nLanguage models both learn and predict one word at a time. The training of the network involves providing sequences of words as input that are processed one at a time where a prediction can be made and learned for each input sequence.\n\nSimilarly, when making predictions, the process can be seeded with one or a few words, then predicted words can be gathered and presented as input on subsequent predictions in order to build up a generated output sequence\n\nTherefore, each model will involve splitting the source text into input and output sequences, such that the model can learn to predict words.\n\n", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# Sample Text being anlayzed:\n\n\n# source text\ndata = \"\"\" Jack and Jill went up the hill\\n\n\t\tTo fetch a pail of water\\n\n\t\tJack fell down and broke his crown\\n\n\t\tAnd Jill came tumbling after\\n \"\"\"", 
            "metadata": {}, 
            "execution_count": 1, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## Model 1: One-Word-In, One-Word-Out Sequences \nWe can start with a very simple model.\n\nGiven one word as input, the model will learn to predict the next word in the sequence.\n\nKeras provides the Tokenizer class that can be used to perform this encoding. First, the Tokenizer is fit on the source text to develop the mapping from words to unique integers. Then sequences of text can be converted to sequences of integers by calling the texts_to_sequences() function.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "from keras.preprocessing.text import Tokenizer\n\n\n# integer encode text\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts([data])\nencoded = tokenizer.texts_to_sequences([data])[0]\n\n# determine the vocabulary size\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: %d' % vocab_size)", 
            "metadata": {}, 
            "execution_count": 2, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stderr", 
                    "output_type": "stream", 
                    "text": "/gpfs/fs01/user/sbb6-28ae32a56257b0-666d72869b6d/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n"
                }, 
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Vocabulary Size: 22\n"
                }
            ]
        }, 
        {
            "source": "encoded[:]", 
            "metadata": {}, 
            "execution_count": 25, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "[2,\n 1,\n 3,\n 4,\n 5,\n 6,\n 7,\n 8,\n 9,\n 10,\n 11,\n 12,\n 13,\n 2,\n 14,\n 15,\n 1,\n 16,\n 17,\n 18,\n 1,\n 3,\n 19,\n 20,\n 21]"
                    }, 
                    "metadata": {}, 
                    "execution_count": 25, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "We add one, because we will need to specify the integer for the largest encoded word as an array index, e.g. words encoded 1 to 21 with array indicies 0 to 21 or 22 positions.\n\nNext, we need to create sequences of words to fit the model with one word as input and one word as output.\n\nRunning this piece shows that we have a total of 24 input-output pairs to train the network.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "import array", 
            "metadata": {}, 
            "execution_count": 26, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# create word -> word sequences\nsequences = list()\nfor i in range(1, len(encoded)):\n\tsequence = encoded[i-1:i+1]\n\tsequences.append(sequence)\nprint('Total Sequences: %d' % len(sequences))\n\n#We can then split the sequences into input (X) and output elements (y). This is straightforward as we only have two columns in the data.\n\n\n\n", 
            "metadata": {}, 
            "execution_count": 27, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Total Sequences: 24\n"
                }
            ]
        }, 
        {
            "source": "sequences[:]", 
            "metadata": {}, 
            "execution_count": 28, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "[[2, 1],\n [1, 3],\n [3, 4],\n [4, 5],\n [5, 6],\n [6, 7],\n [7, 8],\n [8, 9],\n [9, 10],\n [10, 11],\n [11, 12],\n [12, 13],\n [13, 2],\n [2, 14],\n [14, 15],\n [15, 1],\n [1, 16],\n [16, 17],\n [17, 18],\n [18, 1],\n [1, 3],\n [3, 19],\n [19, 20],\n [20, 21]]"
                    }, 
                    "metadata": {}, 
                    "execution_count": 28, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "# split into X and y elements\nsequences = np.array(sequences)\nX, y = sequences[:,0],sequences[:,1]\n", 
            "metadata": {}, 
            "execution_count": 29, 
            "cell_type": "code", 
            "outputs": [
                {
                    "ename": "NameError", 
                    "output_type": "error", 
                    "evalue": "name 'np' is not defined", 
                    "traceback": [
                        "\u001b[0;31m\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-29-e926a2400195>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# split into X and y elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
                    ]
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "We will fit our model to predict a probability distribution across all words in the vocabulary. That means that we need to turn the output element from a single integer into a one hot encoding with a 0 for every word in the vocabulary and a 1 for the actual word that the value. This gives the network a ground truth to aim for from which we can calculate error and update the model.\n\nKeras provides the to_categorical() function that we can use to convert the integer to a one hot encoding while specifying the number of classes as the vocabulary size.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "from keras.utils import to_categorical", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# one hot encode outputs\ny = to_categorical(y, num_classes=vocab_size)", 
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "We are now ready to define the neural network model.\n\nThe model uses a learned word embedding in the input layer. This has one real-valued vector for each word in the vocabulary, where each word vector has a specified length. In this case we will use a 10-dimensional projection. The input sequence contains a single word, therefore the input_length=1.\n\nThe model has a single hidden LSTM layer with 50 units. This is far more than is needed. The output layer is comprised of one neuron for each word in the vocabulary and uses a softmax activation function to ensure the output is normalized to look like a probability.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Embedding, LSTM", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# define model\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 10, input_length=1))\nmodel.add(LSTM(50))\nmodel.add(Dense(vocab_size, activation='softmax'))\nprint(model.summary())", 
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "We will use this same general network structure for each example in this tutorial, with minor changes to the learned embedding layer.\n\nNext, we can compile and fit the network on the encoded text data. Technically, we are modeling a multi-class classification problem (predict the word in the vocabulary), therefore using the categorical cross entropy loss function. We use the efficient Adam implementation of gradient descent and track accuracy at the end of each epoch. The model is fit for 500 training epochs, again, perhaps more than is needed.\n\nThe network configuration was not tuned for this and later experiments; an over-prescribed configuration was chosen to ensure that we could focus on the framing of the language model.", 
            "cell_type": "markdown"
        }, 
        {
            "attachments": {
                "image.png": {
                    "image/png": "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"
                }
            }, 
            "metadata": {}, 
            "source": "![image.png](attachment:image.png)", 
            "cell_type": "markdown"
        }, 
        {
            "source": "\n# compile network\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit network\nmodel.fit(X, y, epochs=500, verbose=2)", 
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "After the model is fit, we test it by passing it a given word from the vocabulary and having the model predict the next word. Here we pass in \u2018Jack\u2018 by encoding it and calling model.predict_classes() to get the integer output for the predicted word. This is then looked up in the vocabulary mapping to give the associated word.\n\nThis process could then be repeated a few times to build up a generated sequence of words.\n\nTo make this easier, we wrap up the behavior in a function that we can call by passing in our model and the seed word.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# evaluate\nin_text = 'Jack'\nprint(in_text)\nencoded = tokenizer.texts_to_sequences([in_text])[0]\nencoded = np.array(encoded)\nyhat = model.predict_classes(encoded, verbose=0)\nfor word, index in tokenizer.word_index.items():\n\tif index == yhat:\n\t\tprint(word)", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "def generate_seq(model, tokenizer, seed_text, n_words):\n\tin_text, result = seed_text, seed_text\n\t# generate a fixed number of words\n\tfor _ in range(n_words):\n\t\t# encode the text as integer\n\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n\t\tencoded = np.array(encoded)\n\t\t# predict a word in the vocabulary\n\t\tyhat = model.predict_classes(encoded, verbose=0)\n\t\t# map predicted word index to word\n\t\tout_word = ''\n\t\tfor word, index in tokenizer.word_index.items():\n\t\t\tif index == yhat:\n\t\t\t\tout_word = word\n\t\t\t\tbreak\n\t\t# append to input\n\t\tin_text, result = out_word, result + ' ' + out_word\n\treturn result", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "print(generate_seq(model, tokenizer, 'Jack', 6))", 
            "metadata": {
                "scrolled": false
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "This is a good first cut language model, but does not take full advantage of the LSTM\u2019s ability to handle sequences of input and disambiguate some of the ambiguous pairwise sequences by using a broader context.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "#  Model 2: Line-by-Line Sequence\n\nAnother approach is to split up the source text line-by-line, then break each line down into a series of words that build up.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "from numpy import array\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# Something like that:\n#X,\t\t\t\t\t\t\t\t\ty\n#_, _, _, _, _, Jack, \t\t\t\tand\n#_, _, _, _, Jack, and \t\t\t\tJill\n#_, _, _, Jack, and, Jill,\t\t\twent\n#_, _, Jack, and, Jill, went,\t\tup\n#_, Jack, and, Jill, went, up,\t\tthe\n#Jack, and, Jill, went, up, the,\t\thill", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "This approach may allow the model to use the context of each line to help the model in those cases where a simple one-word-in-and-out model creates ambiguity.\n\nIn this case, this comes at the cost of predicting words across lines, which might be fine for now if we are only interested in modeling and generating lines of text.\n\nNote that in this representation, we will require a padding of sequences to ensure they meet a fixed length input. This is a requirement when using Keras.\n\nFirst, we can create the sequences of integers, line-by-line by using the Tokenizer already fit on the source text.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "from keras.preprocessing.sequence import pad_sequences\n\n# create line-based sequences\nsequences = list()\nfor line in data.split('\\n'):\n\tencoded = tokenizer.texts_to_sequences([line])[0]\n\tfor i in range(1, len(encoded)):\n\t\tsequence = encoded[:i+1]\n\t\tsequences.append(sequence)\nprint('Total Sequences: %d' % len(sequences))", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "Next, we can pad the prepared sequences. We can do this using the pad_sequences() function provided in Keras. This first involves finding the longest sequence, then using that as the length by which to pad-out all other sequences.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "\n# pad input sequences\nmax_length = max([len(seq) for seq in sequences])\nsequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\nprint('Max Sequence Length: %d' % max_length)", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## Next, we can split the sequences into input and output elements, much like before.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# split into input and output elements\nsequences = array(sequences)\nX, y = sequences[:,:-1],sequences[:,-1]\ny = to_categorical(y, num_classes=vocab_size)", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "The model can then be de\fned as before, except the input sequences are now longer than a\nsingle word. Speci\fcally, they are max length-1 in length, -1 because when we calculated the\nmaximum length of sequences, they included the input and output elements.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# define the model\ndef define_model(vocab_size, max_length):\n    model = Sequential()\n    model.add(Embedding(vocab_size, 10, input_length=max_length-1))\n    model.add(LSTM(50))\n    model.add(Dense(vocab_size, activation='softmax'))\n    # compile network\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # summarize defined model\n    model.summary()\n    plot_model(model, to_file='model.png', show_shapes=True)\n    return model", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "We can use the model to generate new sequences as before. The generate seq() function\ncan be updated to build up an input sequence by adding predictions to the list of input words\neach iteration.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# generate a sequence from a language model\ndef generate_seq(model, tokenizer, max_length, seed_text, n_words):\n    in_text = seed_text\n    # generate a fixed number of words\n    for _ in range(n_words):\n        # encode the text as integer\n        encoded = tokenizer.texts_to_sequences([in_text])[0]\n        # pre-pad sequences to a fixed length\n        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n        # predict probabilities for each word\n        yhat = model.predict_classes(encoded, verbose=0)\n        # map predicted word index to word\n        out_word = ''\n        for word, index in tokenizer.word_index.items():\n            if index == yhat:\n                out_word = word\n                break\n        # append to input\n        in_text += ' ' + out_word\n    return in_text", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "Running the example achieves a better \ft on the source data. The added context has allowed\nthe model to disambiguate some of the examples. There are still two lines of text that start\nwith \\Jack\" that may still be a problem for the network.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# define model\nmodel = define_model(vocab_size, max_length)\n# fit network\nmodel.fit(X, y, epochs=500, verbose=2)\n# evaluate model\n", 
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "print(generate_seq(model, tokenizer, max_length-1, 'Jack', 4))\nprint(generate_seq(model, tokenizer, max_length-1, 'Jill', 4))", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "At the end of the run, we generate two sequences with di\u000berent seed words: Jack and Jill.\nThe \frst generated line looks good, directly matching the source text. The second is a bit\nstrange. This makes sense, because the network only ever saw Jill within an input sequence,\nnot at the beginning of the sequence, so it has forced an output to use the word Jill", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "#", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## Model 3: Two-Words-In, One-Word-Out Sequence", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "We can use an intermediate between the one-word-in and the whole-sentence-in approaches\nand pass in a sub-sequences of words as input. This will provide a trade-o\u000b between the two\nframings allowing new lines to be generated and for generation to be picked up mid line. We will\nuse 3 words as input to predict one word as output. The preparation of the sequences is much\nlike the \frst example, except with di\u000berent o\u000bsets in the source sequence arrays, as follows:", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# encode 2 words -> 1 word\nsequences = list()\nfor i in range(2, len(encoded)):\n    sequence = encoded[i-2:i+1]\n    sequences.append(sequence)\n    print('Total Sequences: %d' % len(sequences))", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# generate a sequence from a language model\ndef generate_seq(model, tokenizer, max_length, seed_text, n_words):\n    in_text = seed_text\n    # generate a fixed number of words\n    for _ in range(n_words):\n        # encode the text as integer\n        encoded = tokenizer.texts_to_sequences([in_text])[0]\n        # pre-pad sequences to a fixed length\n        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n        # predict probabilities for each word\n        yhat = model.predict_classes(encoded, verbose=0)\n        # map predicted word index to word\n        out_word = ''\n        for word, index in tokenizer.word_index.items():\n            if index == yhat:\n                out_word = word\n                break\n        # append to input\n        in_text += ' ' + out_word\n    return in_text\n", 
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# define the model\ndef define_model(vocab_size, max_length):\n    model = Sequential()\n    model.add(Embedding(vocab_size, 10, input_length=max_length-1))\n    model.add(LSTM(50))\n    model.add(Dense(vocab_size, activation='softmax'))\n    # compile network\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # summarize defined model\n    model.summary()\n    plot_model(model, to_file='model.png', show_shapes=True)\n    return model", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "tokenizer = Tokenizer()\ntokenizer.fit_on_texts([data])\nencoded = tokenizer.texts_to_sequences([data])[0]\n# retrieve vocabulary size\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: %d' % vocab_size)\n# encode 2 words -> 1 word\nsequences = list()\nfor i in range(2, len(encoded)):\n    sequence = encoded[i-2:i+1]\n    sequences.append(sequence)\nprint('Total Sequences: %d' % len(sequences))\n# pad sequences\nmax_length = max([len(seq) for seq in sequences])\nsequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\nprint('Max Sequence Length: %d' % max_length)\n# split into input and output elements\nsequences = array(sequences)\nX, y = sequences[:,:-1],sequences[:,-1]\ny = to_categorical(y, num_classes=vocab_size)\n# define model\nmodel = define_model(vocab_size, max_length)\n# fit network\nmodel.fit(X, y, epochs=500, verbose=2)", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# evaluate model\nprint(generate_seq(model, tokenizer, max_length-1, 'Jack and', 5))\nprint(generate_seq(model, tokenizer, max_length-1, 'And Jill', 3))\nprint(generate_seq(model, tokenizer, max_length-1, 'fell down', 5))\nprint(generate_seq(model, tokenizer, max_length-1, 'pail of', 5))", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## Summary\nThe first start of line case generated correctly, but the second did not. The second case was\nan example from the 4th line, which is ambiguous with content from the \frst line. Perhaps a\nfurther expansion to 3 input words would be better. The two mid-line generation examples were\ngenerated correctly, matching the source text.\nWe can see that the choice of how the language model is framed and the requirements on\nhow the model will be used must be compatible. That careful design is required when using\nlanguage models in general, perhaps followed-up by spot testing with sequence generation to\ncon\frm model requirements have been met.", 
            "cell_type": "markdown"
        }
    ], 
    "nbformat_minor": 1
}