{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Python DL_NLP Crash Course  Part 2_Keras_MLP_CNN_LSTM\n\n## Full Day Workshop for user learn Data Science with Python\n### 2017 Dec Timothy CL Lam\nThis is meant for internal usage, part of contents copied externally, not for commercial purpose\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# What is Kera\nKeras is a minimalist Python library for deep learning that can run on top of Theano or\nTensorFlow. It was developed to make developing deep learning models as fast and easy as\npossible for research and development. It runs on Python 2.7 or 3.5 and can seamlessly execute\non GPUs and CPUs given the underlying frameworks. It is released under the permissive MIT\nlicense. Keras was developed and maintained by Fran\u0018cois Chollet, a Google engineer using four\nguiding principles:\n\u0088 Modularity: A model can be understood as a sequence or a graph alone. All the concerns\nof a deep learning model are discrete components that can be combined in arbitrary ways.\n\u0088 Minimalism: The library provides just enough to achieve an outcome, no frills and\nmaximizing readability.\n\u0088 Extensibility: New components are intentionally easy to add and use within the frame-\nwork, intended for developers to trial and explore new ideas.\n\u0088 Python: No separate model \fles with custom \fle formats. Everything is native Python.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "!pip list --isolated", 
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "\u001b[31mDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\u001b[0m\nappdirs (1.4.3)\nasn1crypto (0.23.0)\nastunparse (1.5.0)\nBabel (2.1.1)\nbackports-abc (0.5)\nbackports.shutil-get-terminal-size (1.0.0)\nbackports.ssl-match-hostname (3.4.0.2)\nbackports.weakref (1.0rc1)\nbasemap (1.0.7)\nbeautifulsoup4 (4.4.1)\nbiopython (1.66)\nbitarray (0.8.1)\nbkcharts (0.2)\nbleach (2.0.0)\nbokeh (0.12.6)\nboto3 (1.4.7)\nbotocore (1.7.38)\nbranca (0.2.0)\nbrunel (2.3)\nCacheControl (0.12.3)\ncdsax-jupyter-extensions (0.1)\ncertifi (2017.11.5)\ncffi (1.11.2)\nchardet (3.0.4)\nclick (6.7)\nclick-plugins (1.0.3)\ncligj (0.4.0)\ncognitive-assistant (1.0.52)\ncolorlover (0.2.1)\nconfigparser (3.5.0)\ncryptography (2.1.3)\ncufflinks (0.8.2)\ncycler (0.9.0)\nCython (0.23.4)\ndebtcollector (1.1.0)\ndecorator (4.1.2)\ndescartes (1.0.1)\ndill (0.2.5)\ndlaas-client (0.1.35)\ndocloud (1.0.257)\ndocplex (2.0.15)\ndocutils (0.14)\nentrypoints (0.2.2)\nenum (0.4.6)\nenum34 (1.1.6)\nextension-utils (0.1.57)\nextras (0.0.3)\nfixtures (2.0.0)\nFlask (0.10.1)\nfolium (0.3.0)\nfuncsigs (1.0.2)\nfunctools32 (3.2.3.post2)\nfuture (0.15.2)\nfutures (3.1.1)\ngdbn (0.1)\ngeojson (2.3.0)\ngeopy (1.11.0)\ngnumpy (0.2)\nhtml5lib (0.999999999)\nibm-db (2.0.7)\nibm-db-sa (0.3.3)\nibmdbpy (0.1.4)\nibmos2spark (1.0.1)\nidna (2.6)\ninflection (0.3.1)\nip-associations-python-novaclient-ext (0.1)\nipaddress (1.0.18)\nipdb (0.10.3)\nipykernel (4.6.1)\nipython (5.5.0)\nipython-genutils (0.2.0)\nipython-sql (0.3.8)\nipywidgets (6.0.0)\niso3166 (0.8)\niso8601 (0.1.11)\nitsdangerous (0.24)\nJayDeBeApi (0.2.0)\nJinja2 (2.9.6)\njmespath (0.9.3)\njoblib (0.11)\nJPype1 (0.6.2)\njsonschema (2.6.0)\njupyter-client (5.1.0)\njupyter-core (4.3.0)\njupyter-kernel-gateway (1.2.2)\njupyter-pip (0.3.1)\nKeras (2.0.5)\nkeyring (5.7)\nkeystoneauth1 (2.1.0)\nLasagne (0.1)\nlazy (1.2)\nlinecache2 (1.0.0)\nlxml (3.8.0)\nmapbox (0.14.0)\nMarkdown (2.6.8)\nMarkupSafe (1.0)\nmatplotlib (1.5.0)\nmaven-artifact (0.1.6)\nmistune (0.7.1)\nml-algorithms (1.1.68.post201711160034)\nml-pipeline (1.1.84.post201710060056)\nml-sklearn (0.1.5)\nmock (2.0.0)\nmonotonic (0.4)\nmore-itertools (3.2.0)\nmpld3 (0.3)\nmplleaflet (0.0.5)\nmsgpack-python (0.4.8)\nMySQL-python (1.2.5)\nnbconvert (5.1.1)\nnbformat (4.3.0)\nnetaddr (0.7.18)\nnetifaces (0.10.4)\nnetworkx (1.10)\nnltk (3.2.5)\nnolearn (0.6.0)\nnose (1.3.7)\nnotebook (5.0.0)\nnumexpr (2.4.6)\nnumpy (1.13.3)\nos-diskconfig-python-novaclient-ext (0.1.2)\nos-networksv2-python-novaclient-ext (0.25)\nos-virtual-interfacesv2-python-novaclient-ext (0.19)\noslo.config (3.1.0)\noslo.i18n (3.1.0)\noslo.serialization (2.1.0)\noslo.utils (3.2.0)\npackaging (16.8)\npandas (0.21.0)\npandas-connector (0.1.57)\npandocfilters (1.4.1)\npath.py (8.1.2)\npathlib2 (2.3.0)\npatsy (0.4.1)\npbr (1.8.1)\npexpect (4.2.1)\npickleshare (0.7.4)\nPillow (3.0.0)\npip (9.0.1)\npixiedust (1.0.9)\nplotly (2.0.12)\npolyline (1.3.2)\nprettytable (0.7.2)\nproject-lib (0.0.4)\nprojectnb (0.1.6)\nprompt-toolkit (1.0.15)\nprotobuf (3.0.0b2)\npsycopg2 (2.5.4)\nptyprocess (0.5.2)\npycparser (2.18)\npydotplus (2.0.2)\nPygments (2.2.0)\npyOpenSSL (17.3.0)\npyparsing (2.0.6)\npyproj (1.9.4)\npypyodbc (1.3.4)\npyrax (1.9.5)\npyshp (1.2.3)\npysolr (3.6.0)\npython-dateutil (2.6.1)\npython-keystoneclient (2.0.0)\npython-mimeparse (1.5.1)\npython-novaclient (2.35.0)\npython-subunit (1.2.0)\npython-swiftclient (3.4.0)\npytz (2017.3)\nPyYAML (3.12)\npyzmq (15.1.0)\nQuandl (3.2.0)\nrackspace-auth-openstack (1.3)\nrackspace-novaclient (1.5)\nrax-default-network-flags-python-novaclient-ext (0.3.2)\nrax-scheduled-images-python-novaclient-ext (0.3.1)\nrepository (0.1.361.post201711290213)\nrepository-v3 (0.1.361.post201711290213)\nrepoze.lru (0.6)\nrequests (2.18.4)\ns3transfer (0.1.11)\nscandir (1.6)\nscikit-image (0.11.3)\nscikit-learn (0.19.0)\nscipy (0.19.1)\nseaborn (0.7.1)\nsetuptools (36.5.0)\nsilpa-common (0.3)\nsimplegeneric (0.8.1)\nSimpleITK (1.0.1)\nsimplejson (3.8.1)\nsingledispatch (3.4.0.3)\nsix (1.11.0)\nsklearn-pipeline (0.1.507)\nsoundex (1.1.3)\nSQLAlchemy (1.1.12)\nsqlparse (0.2.3)\nstatsmodels (0.6.1)\nstevedore (1.10.0)\nstreamsx (1.6.0)\nsympy (0.7.6.1)\nsystemml (0.15.0)\ntabulate (0.7.7)\ntensorflow (1.2.1)\nterminado (0.5)\ntestpath (0.3.1)\ntestrepository (0.0.20)\ntesttools (2.1.0)\ntflearn (0.3.2)\nTheano (0.9.0)\ntornado (4.5.1)\ntraceback2 (1.4.0)\ntraitlets (4.3.2)\ntransformation (0.1.124)\nunittest2 (1.1.0)\nuritemplate (3.0.0)\nurllib3 (1.22)\nwatson-developer-cloud (1.0.0)\nwcwidth (0.1.7)\nwebencodings (0.5.1)\nWerkzeug (0.12.2)\nwget (3.2)\nwheel (0.29.0)\nwidgetsnbextension (2.0.0)\nwrapt (1.10.6)\nxgboost (0.6a2)\nxlrd (1.0.0)\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "### Keras is a lightweight API\nRather than providing an implementation of the required\nmathematical operations needed for deep learning it provides a consistent interface to e\u000ecient\nnumerical libraries called backends. Assuming you have both Theano and TensorFlow installed,\nyou can con\fgure the backend used by Keras.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from keras import backend\nprint (backend._BACKEND)\n#KERAS_BACKEND=theano uncheck if intend to change to Theano backend", 
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "tensorflow\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "## Key Step to Keras\nThe focus of Keras is the idea of a model. The main type of model is a sequence of layers called\na Sequential which is a linear stack of layers. You create a Sequential and add layers to it\nin the order that you wish for the computation to be performed. Once de\fned, you compile\nthe model which makes use of the underlying framework to optimize the computation to be\nperformed by your model. In this you can specify the loss function and the optimizer to be used.\nOnce compiled, the model must be \ft to data. This can be done one batch of data at a\ntime or by \fring o\u000b the entire model training regime. This is where all the compute happens.\nOnce trained, you can use your model to make predictions on new data. We can summarize the\nconstruction of deep learning models in Keras as follows:\n1. Define your model. Create a Sequential model and add con\fgured layers.\n2. Compile your model. Specify loss function and optimizers and call the compile()\nfunction on the model.\n3. Fit your model. Train the model on a sample of data by calling the fit() function on\nthe model.\n4. Make predictions. Use the model to generate predictions on new data by calling\nfunctions such as evaluate() or predict() on the model.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Keras Model Life-Cycle\nBelow is an overview of the 5 steps in the neural network model life-cycle in Keras:\n1. De\fne Network.\n2. Compile Network.\n3. Fit Network.\n4. Evaluate Network.\n5. Make Predictions.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "## Sequential API (Light Weight Tunning Logi Flow)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Step 1: Define Network\nThe \frst step is to de\fne your neural network. Neural networks are de\fned in Keras as a\nsequence of layers. The container for these layers is the Sequential class. The \frst step is to\ncreate an instance of the Sequential class. Then you can create your layers and add them in\nthe order that they should be connected. For example, we can do this in two steps: ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense", 
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": []
        }, 
        {
            "source": "model = Sequential()\nmodel.add(Dense(2))\n\n#But we can also do this in one step by creating an array of layers and passing it to the constructor of the Sequential class.\nlayers = [Dense(2)]\nmodel = Sequential(layers)", 
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "evalue": "The first layer in a Sequential model must get an `input_shape` or `batch_input_shape` argument.", 
                    "ename": "ValueError", 
                    "traceback": [
                        "\u001b[0;31m\u001b[0m", 
                        "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-12-06deaaf6044b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#But we can also do this in one step by creating an array of layers and passing it to the constructor of the Sequential class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/Users/timothycllam/Library/DataScienceStudio/dss_home/pyenv/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0;31m# know about its input shape. Otherwise, that's an error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch_input_shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                         raise ValueError('The first layer in a '\n\u001b[0m\u001b[1;32m    452\u001b[0m                                          \u001b[0;34m'Sequential model must '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                                          \u001b[0;34m'get an `input_shape` or '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mValueError\u001b[0m: The first layer in a Sequential model must get an `input_shape` or `batch_input_shape` argument."
                    ], 
                    "output_type": "error"
                }
            ]
        }, 
        {
            "source": "** The first layer in the network must de\fne the number of inputs to expect. **\n\nThe way that this is speci\fed can di\u000ber depending on the network type, but for a Multilayer Perceptron model\nthis is speci\fed by the input dim attribute. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# For example, a small Multilayer Perceptron model with 2 inputs in the visible layer, 5 neurons in the hidden layer\n# and one neuron in the output layer can be dened as:\n\nmodel = Sequential()\nmodel.add(Dense(5, input_dim=2))\nmodel.add(Dense(1))", 
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "Think of a Sequential model as a pipeline with your raw data fed in at the bottom and\npredictions that come out at the top. This is a helpful conception in Keras as concerns that were\ntraditionally associated with a layer can also be split out and added as separate layers\n\nLIKE A TRANGLE CAKE", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Activation\nFor example, activation\nfunctions that transform a summed signal from each neuron in a layer can be extracted and\nadded to the Sequential as a layer-like object called the Activation class.\n\n- ** Regression **: Linear activation function, or linear, and the number of neurons matching\nthe number of outputs.\n\n\n- ** Binary Classification ** (2 class): Logistic activation function, or sigmoid, and one neuron the output layer.\n\n\u0088\n-  ** Multiclass Classification ** (>2 class): Softmax activation function, or softmax, and one output neuron per class value, assuming a one hot encoded output pattern.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "model = Sequential()\nmodel.add(Dense(5, input_dim=2))\nmodel.add(Activation('relu'))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))", 
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "evalue": "name 'Activation' is not defined", 
                    "ename": "NameError", 
                    "traceback": [
                        "\u001b[0;31m\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-15-710475d5c755>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mNameError\u001b[0m: name 'Activation' is not defined"
                    ], 
                    "output_type": "error"
                }
            ]
        }, 
        {
            "source": "The choice of activation function is most important for the output layer as it will de\fne the\nformat that predictions will take. For example, below are some common predictive modeling\nproblem types and the structure and standard activation function that you can use in the output\nlayer:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Step 2. Compile Network ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Once we have de\fned our network, we must compile it. Compilation is an e\u000eciency step. It\ntransforms the simple sequence of layers that we de\fned into a highly e\u000ecient series of matrix\ntransforms in a format intended to be executed on your GPU or CPU, depending on how Keras\nis con\fgured. \n\n** Think of compilation as a precompute step for your network. It is always required\nafter defining a model. **", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Compilation requires a number of parameters to be speci\fed, speci\fcally tailored to training\nyour network. Speci\fcally:\n\n**the optimization algorithm** to use to train the network;\n\n**the loss function ** used to evaluate the network that is minimized by the optimization algorithm. \n\nForexample, below is a case of compiling a de\fned model and specifying the stochastic gradient\ndescent (sgd) optimization algorithm and the mean squared error (mean squared error) loss\nfunction, intended for a regression type problem.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "%pylab inline", 
            "execution_count": 0, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "import dataiku\nfrom dataiku import pandasutils as pdu\nimport pandas as pd", 
            "execution_count": 0, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "# Example: load a DSS dataset as a Pandas dataframe\nmydataset = dataiku.Dataset(\"mydataset\")\nmydataset_df = mydataset.get_dataframe()", 
            "execution_count": 0, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "model.compile(optimizer='sgd', loss='mean_squared_error')", 
            "execution_count": 17, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "** Alternately **, the optimizer can be created and con\fgured before being provided as an argument\nto the compilation step.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "algorithm = SGD(lr=0.1, momentum=0.3)\nmodel.compile(optimizer=algorithm, loss='mean_squared_error')", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "### Optimization Algorithm\n\nThe most common optimization algorithm is stochastic gradient descent, but Keras also\nsupports a suite of other state-of-the-art optimization algorithms that work well with little or\nno con\fguration.\n\n- ** Stochastic Gradient Descent **, or sgd, that requires the tuning of a learning rate and momentum.\n\n\n- ** Adam **, or adam, that requires the tuning of learning rate.\n\n\n- ** RMSprop **, or rmsprop, that requires the tuning of learning rate.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Loss Function\n\nRegression: Mean Squared Error or ** mean squared error. **\n\n\u0088Binary Classi\fcation (2 class): ** Logarithmic Loss, also called cross entropy or\nbinary crossentropy **\n\n\n\u0088Multiclass Classi\fcation (>2 class): ** Multiclass Logarithmic Loss or\ncategorical crossentropy. **", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Metrics\nFinally, you can also specify ** metrics ** to collect while \ftting your model in addition to the\nloss function. Generally, the most useful additional metric to collect is accuracy for classi\fcation\nproblems. The metrics to collect are speci\fed by name in an array.\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "model.compile(optimizer='sgd', loss='mean_squared_error', metrics=['accuracy'])", 
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "## Step 3. Fit Network", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Once the network is compiled, it can be fit, which means adapt the weights on a training dataset.\nFitting the network requires the training data to be speci\fed, both a matrix of input patterns, ** X ** (MATRIX!!!),\nand an array of matching output patterns, ** y **. \n\nThe network is trained using the $$ backpropagation $$\n\nand optimized according to the optimization algorithm and loss function speci\fed\nwhen compiling the model.\n\n\n** The backpropagation algorithm ** requires that the network be trained for a speci\fed number\nof \n\n$$ epochs $$\n\nor exposures to the training dataset. Each epoch can be partitioned into groups\nof input-output pattern pairs called \n\n$$ batches $$. \n\nThis defines the number of patterns that the\nnetwork is exposed to before the weights are updated within an epoch. \n\nIt is also an efficiency optimization, ensuring that not too many input patterns are loaded into memory at a time. A\nminimal example of \ftting a network is as follows:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "history = model.fit(X, y, batch_size=10, epochs=100)", 
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "evalue": "name 'X' is not defined", 
                    "ename": "NameError", 
                    "traceback": [
                        "\u001b[0;31m\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-20-755da63bdedd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
                    ], 
                    "output_type": "error"
                }
            ]
        }, 
        {
            "source": "### TOO MUCH INFORMATION\n\nBy default, a progress bar is displayed on the command line for each epoch. This may create\ntoo much noise for you, or may cause problems for your environment, such as if you are in an\ninteractive notebook or IDE. You can reduce the amount of information displayed to just the\nloss each epoch by setting the \n\n$$ verbose $$\n\nargument to 2. You can turn o\u000b all output by setting\nverbose to 0. For example:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "history = model.fit(X, y, batch_size=10, epochs=100, verbose=0)", 
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "evalue": "name 'X' is not defined", 
                    "ename": "NameError", 
                    "traceback": [
                        "\u001b[0;31m\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-21-c6a53d142b25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
                    ], 
                    "output_type": "error"
                }
            ]
        }, 
        {
            "source": "## Step 4. Evaluate Network\n\nThe model evaluates the loss across all of the test patterns, as well as any other metrics\nspeci\fed when the model was compiled, like classi\fcation accuracy. A list of evaluation metrics\nis returned. For example, for a model compiled with the accuracy metric, we could evaluate it\non a new dataset as follows:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "loss, accuracy = model.evaluate(X, y)\n\n#As with fitting the network, verbose output is provided to give an idea of the evaluating the model. We can turn this o\u000b by setting the verbose argument to 0.\n\nloss, accuracy = model.evaluate(X, y, verbose=0)", 
            "execution_count": 24, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "evalue": "unexpected indent (<ipython-input-24-79dc64a86d0e>, line 4)", 
                    "ename": "IndentationError", 
                    "traceback": [
                        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-79dc64a86d0e>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    by setting the verbose argument to 0.\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
                    ], 
                    "output_type": "error"
                }
            ]
        }, 
        {
            "source": "## Step 5. Make Predictions\n\nOnce we are satis\fed with the performance of our \ft model, we can use it to make predictions\non new data. This is as easy as calling the predict() function on the model with an array of\nnew input patterns. For example:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "predictions = model.predict(X)", 
            "execution_count": 25, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "evalue": "name 'X' is not defined", 
                    "ename": "NameError", 
                    "traceback": [
                        "\u001b[0;31m\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-25-b850e5c4a95e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
                    ], 
                    "output_type": "error"
                }
            ]
        }, 
        {
            "source": "For a multiclass classi\fcation problem, the results may be in the form of an array of\nprobabilities (assuming a one hot encoded output variable) that may need to be converted to a\nsingle class output prediction using the \n\n$$ argmax() $$\n\nNumPy function. Alternately, for classi\fcation\nproblems, we can use the predict classes() function that will automatically convert uncrisp\npredictions to crisp integer class values.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "predictions = model.predict_classes(X)", 
            "execution_count": 27, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "evalue": "name 'X' is not defined", 
                    "ename": "NameError", 
                    "traceback": [
                        "\u001b[0;31m\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-27-ce7ca631b31a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
                    ], 
                    "output_type": "error"
                }
            ]
        }, 
        {
            "source": "# Keras Functional Model (Flexibile Real Code)\n\nThe sequential API allows you to create models layer-by-layer for most problems. It is limited\nin that it does not allow you to create models that share layers or have multiple inputs or\noutputs. The functional API in Keras is an alternate way of creating models that o\u000bers a lot\nmore flexibility, including creating more complex models.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "It speci\fcally allows you to de\fne multiple input or output models as well as models that\nshare layers. More than that, it allows you to de\fne ad hoc \n\n** acyclic network graphs **\n\nModels are de\fned by creating instances of layers and connecting them directly to each other in pairs, then\nde\fning a Model that speci\fes the layers to act as the input and output to the model. Let's\nlook at the three unique aspects of Keras functional API in turn:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Defining Input ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Unlike the Sequential model, you must create and de\fne a standalone Input layer that specifies\n\n** the shape of input data **\n\nThe input layer takes a \n\n$$ shape $$\n\nargument that is a $$ tuple $$ \n\nthat indicates ** the dimensionality of the input data **. \n\n// When input data is ** one-dimensional **, such as for a Multilayer Perceptron, the shape must explicitly leave room for the shape of the mini-batch size used when splitting the data when training the network. Therefore, the shape tuple is always defined with a hanging last dimension (2,), for example:  //", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from keras.layers import Input\nvisible = Input(shape=(2,))", 
            "execution_count": 29, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "## Connecting Layer\nThe layers in the model are connected pairwise. This is done by specifying where the input\ncomes from when de\fning each new layer. A bracket notation is used, such that after the layer\nis created, the layer from which the input to the current layer comes from is speci\fed. Let's\nmake this clear with a short example. We can create the input layer as above, then create a\nhidden layer as a Dense that receives input only from the input layer.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from keras.layers import Input\nfrom keras.layers import Dense\nvisible = Input(shape=(2,))\nhidden = Dense(2)(visible)\n\n#Note it is the visible after the creation of the Dense layer that connects the input layer's\n#output as the input to the Dense hidden layer. It is this way of connecting layers piece by piece\n#that gives the functional API its \n#flexibility.", 
            "execution_count": 32, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "### Creating the Model\n\nAfter creating all of your model layers and connecting them together, you must define the model.\nAs with the Sequential API, the model is the thing you can ** summarize, fit, evaluate **, and use to\nmake predictions.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nvisible = Input(shape=(2,))\nhidden = Dense(2)(visible)\nmodel = Model(inputs=visible, outputs=hidden)\n\n#Keras provides a Model class that you can use to create a model from your\n#created layers. It requires that you only specify the input and output layers.", 
            "execution_count": 33, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "## Multilayer Preceptron (MLP)\nwe de\fne a Multilayer Perceptron model for binary classi\fcation. The model\nhas 10 inputs, 3 hidden layers with 10, 20, and 10 neurons, and an output layer with 1 output.\nRecti\fed linear activation functions are used in each hidden layer and a sigmoid activation\nfunction is used in the output layer, for binary classi\fcation.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nvisible = Input(shape=(10,))\nhidden1 = Dense(10, activation = 'relu')(visible)\nhidden2 = Dense(20, activation = 'relu')(hidden1)\nhidden3 = Dense(10, activation = 'relu')(hidden2)\noutput = Dense(1, activation = 'sigmoid')(hidden3)\nmodel = Model(inputs=visible, outputs=output)\nmodel.summary()\n\n\n# plot graph\nplot_model(model, to_file='multilayer_perceptron_graph.png')\n\n\n#Note, creating plots of Keras models requires that you install pydot and pygraphviz (the\n#graphviz library and the python wrapper).", 
            "execution_count": 46, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_11 (InputLayer)        (None, 10)                0         \n_________________________________________________________________\ndense_32 (Dense)             (None, 10)                110       \n_________________________________________________________________\ndense_33 (Dense)             (None, 20)                220       \n_________________________________________________________________\ndense_34 (Dense)             (None, 10)                210       \n_________________________________________________________________\ndense_35 (Dense)             (None, 1)                 11        \n=================================================================\nTotal params: 551\nTrainable params: 551\nNon-trainable params: 0\n_________________________________________________________________\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "## Convolution Neural Network (CNN)\n\nThe model receives black and white 64 x 64 images as input, then has a sequence of two convolutional and\npooling layers as feature extractors, followed by a fully connected layer to interpret the features\nand an output layer with a sigmoid activation for two-class predictions.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.pooling import MaxPooling2D\nvisible = Input(shape=(64,64,1))\nconv1 = Conv2D(32, kernel_size =4, activation = 'relu')(visible)\npool1 = MaxPooling2D(pool_size=(2,2))(conv1)\nconv2 = Conv2D(16, kernel_size =4, activation = 'relu')(pool1)\npool2 = MaxPooling2D(pool_size=(2,2))(conv2)\nhidden1 = Dense(10, activation='relu')(pool2)\noutput = Dense(1, activation='sigmoid')(hidden1)\nmodel = Model(inputs=visible, outputs=output)\n\nmodel.summary()", 
            "execution_count": 59, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_18 (InputLayer)        (None, 64, 64, 1)         0         \n_________________________________________________________________\nconv2d_13 (Conv2D)           (None, 61, 61, 32)        544       \n_________________________________________________________________\nmax_pooling2d_13 (MaxPooling (None, 30, 30, 32)        0         \n_________________________________________________________________\nconv2d_14 (Conv2D)           (None, 27, 27, 16)        8208      \n_________________________________________________________________\nmax_pooling2d_14 (MaxPooling (None, 13, 13, 16)        0         \n_________________________________________________________________\ndense_48 (Dense)             (None, 13, 13, 10)        170       \n_________________________________________________________________\ndense_49 (Dense)             (None, 13, 13, 1)         11        \n=================================================================\nTotal params: 8,933\nTrainable params: 8,933\nNon-trainable params: 0\n_________________________________________________________________\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "\nfrom keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.pooling import MaxPooling2D\nvisible = Input(shape=(64,64,1))\nconv1 = Conv2D(32, kernel_size=4, activation='relu')(visible)\npool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\nconv2 = Conv2D(16, kernel_size=4, activation='relu')(pool1)\npool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\nhidden1 = Dense(10, activation='relu')(pool2)\noutput = Dense(1, activation='sigmoid')(hidden1)\nmodel = Model(inputs=visible, outputs=output)\n#Summary of the model\nmodel.summary()", 
            "execution_count": 56, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_16 (InputLayer)        (None, 64, 64, 1)         0         \n_________________________________________________________________\nconv2d_9 (Conv2D)            (None, 61, 61, 32)        544       \n_________________________________________________________________\nmax_pooling2d_9 (MaxPooling2 (None, 30, 30, 32)        0         \n_________________________________________________________________\nconv2d_10 (Conv2D)           (None, 27, 27, 16)        8208      \n_________________________________________________________________\nmax_pooling2d_10 (MaxPooling (None, 13, 13, 16)        0         \n_________________________________________________________________\ndense_44 (Dense)             (None, 13, 13, 10)        170       \n_________________________________________________________________\ndense_45 (Dense)             (None, 13, 13, 1)         11        \n=================================================================\nTotal params: 8,933\nTrainable params: 8,933\nNon-trainable params: 0\n_________________________________________________________________\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "## Recurrent Neural Network (Long Short Term Memory/LSTM)\nIn this section, we will de\fne a long short-term memory recurrent neural network for sequence\nclassi\fcation. \n\nThe model expects 100 time steps of one feature as input. The model has a single\nLSTM hidden layer to extract features from the sequence, followed by a fully connected layer to\ninterpret the LSTM output, followed by an output layer for making binary predictions.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers.recurrent import LSTM\nvisible = Input(shape=(100,1))\nhidden1 = LSTM(10)(visible)\nhidden2 = Dense(10, activation='relu')(hidden1)\noutput = Dense(1, activation='sigmoid')(hidden2)\nmodel = Model(inputs=visible, outputs=output)\nmodel.summary()\n", 
            "execution_count": 60, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_19 (InputLayer)        (None, 100, 1)            0         \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 10)                480       \n_________________________________________________________________\ndense_50 (Dense)             (None, 10)                110       \n_________________________________________________________________\ndense_51 (Dense)             (None, 1)                 11        \n=================================================================\nTotal params: 601\nTrainable params: 601\nNon-trainable params: 0\n_________________________________________________________________\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "name": "python2", 
            "language": "python", 
            "display_name": "Python 2 with Spark 1.6 (Unsupported)"
        }, 
        "creator": "admin", 
        "language_info": {
            "version": "2.7.11", 
            "name": "python", 
            "pygments_lexer": "ipython2", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }
        }, 
        "tags": []
    }
}
