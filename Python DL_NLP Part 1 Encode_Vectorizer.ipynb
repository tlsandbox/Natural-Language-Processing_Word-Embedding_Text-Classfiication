{
    "cells": [
        {
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Python DL_NLP Crash Course_Part 1 Encode_Vectorizer\n\n## Full Day Workshop for user learn Data Science with Python\n### 2017 Dec Timothy CL Lam\nThis is meant for internal usage, part of contents copied externally, not for commercial purpose\n"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Background: From Linguistic to NLP\n\n## Linguistics\nLinguistics is the scienti\fc study of language, including its grammar, semantics, and phonetics.\nClassical linguistics involved devising and evaluating rules of language. Great progress was made\non formal methods for syntax and semantics, but for the most part, the interesting problems in\nnatural language understanding resist clean mathematical formalisms.\nBroadly, a linguist is anyone who studies language, but perhaps more colloquially, a self-\nde\fning linguist may be more focused on being out in the \feld. Mathematics is the tool of\nscience. Mathematicians working on natural language may refer to their study as mathematical\nlinguistics, focusing exclusively on the use of discrete mathematical formalisms and theory for\nnatural language (e.g. formal languages and automata theory).\n\n## Computational Linguistics\nComputational linguistics is the modern study of linguistics using the tools of computer science.\nYesterday's linguistics may be today's computational linguist as the use of computational tools\nand thinking has overtaken most \felds of study.\nComputational linguistics is the study of computer systems for understanding and\ngenerating natural language. ... One natural function for computational linguistics\nwould be the testing of grammars proposed by theoretical linguists. Large data and fast computers mean that new and di\u000berent things can be discovered from\nlarge datasets of text by writing and running software. In the 1990s, statistical methods and\nstatistical machine learning began to and eventually replaced the classical top-down rule-based\napproaches to language, primarily because of their better results, speed, and robustness. The\nstatistical approach to studying natural language now dominates the \feld; it may de\fne the\n\feld.\n \n ## Statistical Natural Language Processing\nComputational linguistics also became known by the name of natural language process, or\nNLP, to re\nect the more engineer-based or empirical approach of the statistical methods. The\nstatistical dominance of the \feld also often leads to NLP being described as Statistical Natural\nLanguage Processing, perhaps to distance it from the classical computational linguistics methods."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# WORD "
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# What is One Hot Encoding?\n\nA one hot encoding is a representation of categorical variables as binary vectors.\n\nThis first requires that the categorical values be mapped to integer values.\n\nThen, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1.\n\n## Why Use a One Hot Encoding?\n\nA one hot encoding allows the representation of categorical data to be more expressive.\n\nMany machine learning algorithms cannot work with categorical data directly. The categories must be converted into numbers. This is required for both input and output variables that are categorical.\n\n## Integer Encoding\nWe could use an integer encoding directly, rescaled where needed. This may work for problems where there is a natural ordinal relationship between the categories, and in turn the integer values, such as labels for temperature \u2018cold\u2019, warm\u2019, and \u2018hot\u2019."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Manual Encoding \nIn this example, we will assume the case where we have an example string of characters of alphabet letters, but the example sequence does not cover all possible examples."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "text": "hello world\n[7, 4, 11, 11, 14, 26, 22, 14, 17, 11, 3]\n[[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\nh\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "source": "from numpy import argmax\n# define input string\ndata = 'hello world'\nprint(data)\n# define universe of possible input values\nalphabet = 'abcdefghijklmnopqrstuvwxyz '\n# define a mapping of chars to integers\nchar_to_int = dict((c, i) for i, c in enumerate(alphabet))\nint_to_char = dict((i, c) for i, c in enumerate(alphabet))\n# integer encode input data\ninteger_encoded = [char_to_int[char] for char in data]\nprint(integer_encoded)\n# one hot encode\nonehot_encoded = list()\nfor value in integer_encoded:\n\tletter = [0 for _ in range(len(alphabet))]\n\tletter[value] = 1\n\tonehot_encoded.append(letter)\nprint(onehot_encoded)\n# invert encoding\ninverted = int_to_char[argmax(onehot_encoded[0])]\nprint(inverted)", 
            "execution_count": 1
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Encode with Sckit Learn\nNow that we have seen how to roll our own one hot encoding from scratch, let\u2019s see how we can use the scikit-learn library to perform this mapping automatically for cases where the input sequence fully captures the expected range of input values:\n\ncold, cold, warm, cold, hot, hot, warm, cold, warm, hot"
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "text": "['cold' 'cold' 'warm' 'cold' 'hot' 'hot' 'warm' 'cold' 'warm' 'hot']\n[0 0 2 0 1 1 2 0 2 1]\n[[ 1.  0.  0.]\n [ 1.  0.  0.]\n [ 0.  0.  1.]\n [ 1.  0.  0.]\n [ 0.  1.  0.]\n [ 0.  1.  0.]\n [ 0.  0.  1.]\n [ 1.  0.  0.]\n [ 0.  0.  1.]\n [ 0.  1.  0.]]\n['cold']\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "source": "from numpy import array\nfrom numpy import argmax\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n# define example\ndata = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\nvalues = array(data)\nprint(values)\n# integer encode\nlabel_encoder = LabelEncoder()\ninteger_encoded = label_encoder.fit_transform(values)\nprint(integer_encoded)\n# binary encode\nonehot_encoder = OneHotEncoder(sparse=False)\ninteger_encoded = integer_encoded.reshape(len(integer_encoded), 1)\nonehot_encoded = onehot_encoder.fit_transform(integer_encoded)\nprint(onehot_encoded)\n# invert first example\ninverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\nprint(inverted)", 
            "execution_count": 3
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Argmax & Index\nBy default, the OneHotEncoder class will return a more efficient sparse encoding. This may not be suitable for some applications, such as use with the Keras deep learning library. In this case, we disabled the sparse return type by setting the sparse=False argument.\n\nIf we receive a prediction in this 3-value one hot encoding, we can easily invert the transform back to the original label.\n\nFirst, we can use the argmax() NumPy function to locate the index of the column with the largest value. This can then be fed to the LabelEncoder to calculate an inverse transform back to a text label\n\nThis is demonstrated at the end of the example with the inverse transform of the first one hot encoded example back to the label value \u2018cold\u2019.\n\nAgain, note that input was formatted for readability."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# One Hot Encoding with Kera \n\nYou may have a sequence that is already integer encoded.\n\nYou could work with the integers directly, after some scaling. Alternately, you can one hot encode the integers directly. This is important to consider if the integers do not have a real ordinal relationship and are really just placeholders for labels.\n\nThe Keras library offers a function called to_categorical() that you can use to one hot encode integer data.\n\nIn this example, we have 4 integer values [0, 1, 2, 3] and we have the input sequence of the following 10 numbers:"
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "text": "Using TensorFlow backend.\n", 
                    "output_type": "stream", 
                    "name": "stderr"
                }, 
                {
                    "text": "[1 3 2 0 3 2 2 1 0 1]\n[[ 0.  1.  0.  0.]\n [ 0.  0.  0.  1.]\n [ 0.  0.  1.  0.]\n [ 1.  0.  0.  0.]\n [ 0.  0.  0.  1.]\n [ 0.  0.  1.  0.]\n [ 0.  0.  1.  0.]\n [ 0.  1.  0.  0.]\n [ 1.  0.  0.  0.]\n [ 0.  1.  0.  0.]]\n1\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "source": "from numpy import array\nfrom numpy import argmax\nfrom keras.utils import to_categorical\n# define example\ndata = [1, 3, 2, 0, 3, 2, 2, 1, 0, 1]\ndata = array(data)\nprint(data)\n# one hot encode\nencoded = to_categorical(data)\nprint(encoded)\n# invert encoding\ninverted = argmax(encoded[0])\nprint(inverted)", 
            "execution_count": 5
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Framing Language Modeling\n\nA statistical language model is learned from raw text and predicts the probability of the next word in the sequence given the words already present in the sequence.\n\nLanguage models are a key component in larger models for challenging natural language processing problems, like machine translation and speech recognition. They can also be developed as standalone models and used for generating new sequences that have the same statistical properties as the source text.\n\nLanguage models both learn and predict one word at a time. The training of the network involves providing sequences of words as input that are processed one at a time where a prediction can be made and learned for each input sequence.\n\nSimilarly, when making predictions, the process can be seeded with one or a few words, then predicted words can be gathered and presented as input on subsequent predictions in order to build up a generated output sequence\n\nTherefore, each model will involve splitting the source text into input and output sequences, such that the model can learn to predict words.\n\nThere are many ways to frame the sequences from a source text for language modeling.\n\nIn this tutorial, we will explore 3 different ways of developing word-based language models in the Keras deep learning library.\n\nThere is no single best approach, just different framings that may suit different applications.\n\n"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# TEXT"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Prepare Text with Keras"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "You cannot feed raw text directly into deep learning models.\n\nText data must be encoded as numbers to be used as input or output for machine learning and deep learning models.\n\nThe Keras deep learning library provides some basic tools to help you prepare your text data.\n\nIn this tutorial, you will discover how you can use Keras to prepare your text data.\n\nAfter completing this tutorial, you will know:\n\nAbout the convenience methods that you can use to quickly prepare text data.\nThe Tokenizer API that can be fit on training data and used to encode training, validation, and test documents.\nThe range of 4 different document encoding schemes offered by the Tokenizer API.\n\n- Split words with text_to_word_sequence.\n- Encoding with one_hot.\n- Hash Encoding with hashing_trick.\n- Tokenizer API\n\n"
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "text": "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "source": "from keras.preprocessing.text import text_to_word_sequence\n# define the document\ntext = 'The quick brown fox jumped over the lazy dog.'\n# tokenize the document\nresult = text_to_word_sequence(text)\nprint(result)", 
            "execution_count": 14
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "A good first step when working with text is to split it into words.\n\nWords are called tokens and the process of splitting text into tokens is called tokenization.\n\nKeras provides the text_to_word_sequence() function that you can use to split text into a list of words.\n\nBy default, this function automatically does 3 things:\n\nSplits words by space (split=\u201d \u201c).\nFilters out punctuation (filters=\u2019!\u201d#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\u2019).\nConverts text to lowercase (lower=True).\nYou can change any of these defaults by passing arguments to the function.\n\nBelow is an example of using the text_to_word_sequence() function to split a document (in this case a simple string) into a list of words."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "from keras.preprocessing.text import text_to_word_sequence\n# define the document\ntext = 'The quick brown fox jumped over the lazy dog.'\n# tokenize the document\nresult = text_to_word_sequence(text)\nprint(result)", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Keras provides the one_hot() function that you can use to tokenize and integer encode a text document in one step. The name suggests that it will create a one-hot encoding of the document, which is not the case.\n\nInstead, the function is a wrapper for the hashing_trick() function described in the next section. The function returns an integer encoded version of the document. The use of a hash function means that there may be collisions and not all words will be assigned unique integer values.\n\nAs with the text_to_word_sequence() function in the previous section, the one_hot() function will make the text lower case, filter out punctuation, and split words based on white space.\n\nIn addition to the text, the vocabulary size (total words) must be specified. This could be the total number of words in the document or more if you intend to encode additional documents that contains additional words. The size of the vocabulary defines the hashing space from which words are hashed. Ideally, this should be larger than the vocabulary by some percentage **(perhaps 25%)** to minimize the number of collisions. By default, the \u2018hash\u2019 function is used, although as we will see in the next section, alternate hash functions can be specified when calling the hashing_trick() function directly.\n\n_* We can use the text_to_word_sequence() function from the previous section to split the document into words and then use a set to represent only the unique words in the document. The size of this set can be used to estimate the size of the vocabulary for one document. _*"
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "text": "8\n[8.0, 5.0, 4.0, 5.0, 2.0, 4.0, 8.0, 9.0, 4.0]\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "source": "\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.text import text_to_word_sequence\n# define the document\ntext = 'The quick brown fox jumped over the lazy dog.'\n# estimate the size of the vocabulary\nwords = set(text_to_word_sequence(text))\nvocab_size = len(words)\nprint(vocab_size)\n# integer encode the document\nresult = one_hot(text, round(vocab_size*1.3))\nprint(result)", 
            "execution_count": 27
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "A limitation of integer and count base encodings is that they must maintain a vocabulary of words and their mapping to integers.\n\nAn alternative to this approach is to use a $$ one-way hash function $$ \n\nto convert words to integers. This avoids the need to keep track of a vocabulary, which is faster and requires less memory.\n\nKeras provides the hashing_trick() function that tokenizes and then integer encodes the document, just like the one_hot() function. It provides more flexibility, allowing you to specify the hash function as either \u2018hash\u2019 (the default) or other hash functions such as the built in md5 function or your own function."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [
                {
                    "traceback": [
                        "\u001b[0;31m\u001b[0m", 
                        "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-25-9e144f99a570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Below is an example of integer encoding a document using the md5 hash function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhashing_trick\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashing_trick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'md5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mImportError\u001b[0m: cannot import name hashing_trick"
                    ], 
                    "output_type": "error", 
                    "evalue": "cannot import name hashing_trick", 
                    "ename": "ImportError"
                }
            ], 
            "source": "#Below is an example of integer encoding a document using the md5 hash function\nfrom keras.preprocessing.text import hashing_trick\nresult = hashing_trick(text,round(vocab_size*1.3), hash_function='md5')\nprint result", 
            "execution_count": 25
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Tokenizer API\n\nSo far we have looked at one-off convenience methods for preparing text with Keras.\n\nKeras provides a more sophisticated API for preparing text that can be fit and reused to prepare multiple text documents. This may be the preferred approach for large projects.\n\nKeras provides the Tokenizer class for preparing text documents for deep learning. The Tokenizer must be constructed and then fit on either raw text documents or integer encoded text documents.\n\n"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "text": "[[ 0.  0.  1.  1.  0.  0.  0.  0.  0.]\n [ 0.  1.  0.  0.  1.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  1.  1.  0.  0.]\n [ 0.  1.  0.  0.  0.  0.  0.  1.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "source": "from keras.preprocessing.text import Tokenizer\n# define 5 documents\ndocs = ['Well done!',\n\t\t'Good work',\n\t\t'Great effort',\n\t\t'nice work',\n\t\t'Excellent!']\n# create the tokenizer\nt = Tokenizer()\n# fit the tokenizer on the documents\nt.fit_on_texts(docs)\n# integer encode documents\nencoded_docs = t.texts_to_matrix(docs, mode='count')\nprint(encoded_docs)", 
            "execution_count": 34
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "text": "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "source": "print(t.word_counts)", 
            "execution_count": 33
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "text": "5\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "source": "print t.document_count ", 
            "execution_count": 32
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "text": "{'great': 5, 'good': 4, 'work': 1, 'well': 2, 'done': 3, 'excellent': 8, 'effort': 6, 'nice': 7}\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "source": "print t.word_index", 
            "execution_count": 30
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "text": "{'great': 1, 'good': 1, 'work': 2, 'well': 1, 'done': 1, 'excellent': 1, 'effort': 1, 'nice': 1}\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "source": "print t.word_docs", 
            "execution_count": 29
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Prepare Text with SckitLearn"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Word Counts with CountVectorizer\n\nThe CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.\n\nYou can use it as follows:\n\nCreate an instance of the CountVectorizer class.\nCall the fit() function in order to learn a vocabulary from one or more documents.\nCall the transform() function on one or more documents as needed to encode each as a vector.\nAn encoded vector is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document.\n\nBecause these vectors will contain a lot of zeros, we call them sparse. Python provides an efficient way of handling sparse vectors in the scipy.sparse package.\n\nThe vectors returned from a call to transform() will be sparse vectors, and you can transform them back to numpy arrays to look and better understand what is going on by calling the toarray() function.\n\nBelow is an example of using the CountVectorizer to tokenize, build a vocabulary, and then encode a document."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "text": "{u'brown': 0, u'lazy': 4, u'jumped': 3, u'over': 5, u'fox': 2, u'dog': 1, u'quick': 6, u'the': 7}\n(1, 8)\n<class 'scipy.sparse.csr.csr_matrix'>\n[[1 1 1 1 1 1 1 2]]\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "source": "from sklearn.feature_extraction.text import CountVectorizer\n# list of text documents\ntext = [\"The quick brown fox jumped over the lazy dog.\"]\n# create the transform\nvectorizer = CountVectorizer()\n# tokenize and build vocab\nvectorizer.fit(text)\n# summarize\nprint(vectorizer.vocabulary_)\n# encode document\nvector = vectorizer.transform(text)\n# summarize encoded vector\nprint(vector.shape)\nprint(type(vector))\nprint(vector.toarray())", 
            "execution_count": 36
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "text": "{u'brown': 0, u'lazy': 4, u'jumped': 3, u'over': 5, u'fox': 2, u'dog': 1, u'quick': 6, u'the': 7}\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "source": "print(vectorizer.vocabulary_)", 
            "execution_count": 37
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## TF-IDF\n\nWord counts are a good starting point, but are very basic.\n\nOne issue with simple counts is that some words like \u201cthe\u201d will appear many times and their large counts will not be very meaningful in the encoded vectors.\n\nAn alternative is to calculate word frequencies, and by far the most popular method is called TF-IDF. This is an acronym than stands for \u201cTerm Frequency \u2013 Inverse Document\u201d Frequency which are the components of the resulting scores assigned to each word.\n\nTerm Frequency: This summarizes how often a given word appears within a document.\nInverse Document Frequency: This downscales words that appear a lot across documents.\nWithout going into the math, TF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents.\n\nThe TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents. Alternately, if you already have a learned CountVectorizer, you can use it with a TfidfTransformer to just calculate the inverse document frequencies and start encoding documents.\n\nThe same create, fit, and transform process is used as with the CountVectorizer.\n\nBelow is an example of using the TfidfVectorizer to learn vocabulary and inverse document frequencies across 3 small documents and then encode one of those documents.m"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "text": "{u'brown': 0, u'lazy': 4, u'jumped': 3, u'over': 5, u'fox': 2, u'dog': 1, u'quick': 6, u'the': 7}\n[ 1.69314718  1.28768207  1.28768207  1.69314718  1.69314718  1.69314718\n  1.69314718  1.        ]\n(1, 8)\n[[ 0.36388646  0.27674503  0.27674503  0.36388646  0.36388646  0.36388646\n   0.36388646  0.42983441]]\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "source": "from sklearn.feature_extraction.text import TfidfVectorizer\n# list of text documents\ntext = [\"The quick brown fox jumped over the lazy dog.\",\n\t\t\"The dog.\",\n\t\t\"The fox\"]\n# create the transform\nvectorizer = TfidfVectorizer()\n# tokenize and build vocab\nvectorizer.fit(text)\n# summarize\nprint(vectorizer.vocabulary_)\nprint(vectorizer.idf_)\n# encode document\nvector = vectorizer.transform([text[0]])\n# summarize encoded vector\nprint(vector.shape)\nprint(vector.toarray())", 
            "execution_count": 39
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Hashing with HashingVectorizer\n\nCounts and frequencies can be very useful, but one limitation of these methods is that the vocabulary can become very large.\n\nThis, in turn, will require large vectors for encoding documents and impose large requirements on memory and slow down algorithms.\n\nA clever work around is to use a one way hash of words to convert them to integers. The clever part is that no vocabulary is required and you can choose an arbitrary-long fixed length vector. A downside is that the hash is a one-way function so there is no way to convert the encoding back to a word (which may not matter for many supervised learning tasks).\n\nThe HashingVectorizer class implements this approach that can be used to consistently hash words, then tokenize and encode documents as needed.\n\nThe example below demonstrates the HashingVectorizer for encoding a single document.\n\nAn arbitrary fixed-length vector size of 20 was chosen. This corresponds to the range of the hash function, where small values (like 20) may result in hash collisions. Remembering back to compsci classes, I believe there are heuristics that you can use to pick the hash length and probability of collision based on estimated vocabulary size.\n\nNote that this vectorizer does not require a call to fit on the training data documents. Instead, after instantiation, it can be used directly to start encoding documents."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "text": "(1, 20)\n[[ 0.          0.          0.          0.          0.          0.33333333\n   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n   0.          0.          0.         -0.33333333  0.          0.\n  -0.66666667  0.        ]]\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "source": "from sklearn.feature_extraction.text import HashingVectorizer\n# list of text documents\ntext = [\"The quick brown fox jumped over the lazy dog.\"]\n# create the transform\nvectorizer = HashingVectorizer(n_features=20)\n# encode document\nvector = vectorizer.transform(text)\n# summarize encoded vector\nprint(vector.shape)\nprint(vector.toarray())", 
            "execution_count": 40
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.1", 
            "name": "python2-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "version": "2.7.11", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }, 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "file_extension": ".py", 
            "name": "python", 
            "pygments_lexer": "ipython2"
        }
    }, 
    "nbformat_minor": 1
}
